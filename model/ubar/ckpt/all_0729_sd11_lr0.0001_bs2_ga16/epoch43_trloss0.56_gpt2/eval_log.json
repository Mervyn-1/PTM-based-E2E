INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  area name type choice <eos_a> <sos_r>  there are [value_choice] [value_type]  in the [value_area] . [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  name address <eos_a> <sos_r>  another college is called [value_name] , another college is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  area name type choice <eos_a> <sos_r>  there are [value_choice] [value_type]  in the [value_area] . [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  of town. [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:inference time: 85.95 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 88.9  success: 80.7  bleu: 17.9    score: 102.7
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Running eavl on test
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 70.13 min
INFO:root:Scoring time: 0.25 min
INFO:root:validation [CTR] match: 87.4  success: 77.7  bleu: 18.2    score: 100.8
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
ERROR:transformers.tokenization_utils:Using cls_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using cls_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using mask_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using mask_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using pad_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using pad_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using sep_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using sep_token, but it is not set yet.
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
ERROR:transformers.tokenization_utils:Using cls_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using cls_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using mask_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using mask_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using pad_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using pad_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using sep_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using sep_token, but it is not set yet.
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
ERROR:transformers.tokenization_utils:Using cls_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using cls_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using mask_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using mask_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using pad_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using pad_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using sep_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using sep_token, but it is not set yet.
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:inference time: 22.76 min
INFO:root:Scoring time: 0.25 min
INFO:root:validation [CTR] match: 42.7  success: 3.3  bleu: 3.6    score: 26.6
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
ERROR:transformers.tokenization_utils:Using cls_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using cls_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using mask_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using mask_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using pad_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using pad_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using sep_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using sep_token, but it is not set yet.
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  area name type choice <eos_a> <sos_r>  there are [value_choice] [value_type]  in the [value_area] . [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  name address <eos_a> <sos_r>  another college is called [value_name] , another college is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is called [value_name] , another is
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  area name type choice <eos_a> <sos_r>  there are [value_choice] [value_type]  in the [value_area] . [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 37.77 min
INFO:root:Scoring time: 0.26 min
INFO:root:validation [CTR] match: 87.8  success: 78.4  bleu: 17.4    score: 100.4
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Running eavl on test
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice] . [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:inference time: 38.46 min
INFO:root:Scoring time: 0.26 min
INFO:root:validation [CTR] match: 86.4  success: 75.6  bleu: 17.7    score: 98.7
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_r>  yes, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, what would you like to do? [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  yes, [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the north. there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the [value_area] . [value_choice]  are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 19.49 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 97.1  success: 94.8  bleu: 27.8    score: 123.8
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
ERROR:transformers.tokenization_utils:Using cls_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using cls_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using mask_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using mask_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using pad_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using pad_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using sep_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using sep_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using cls_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using cls_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using mask_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using mask_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using pad_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using pad_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using sep_token, but it is not set yet.
ERROR:transformers.tokenization_utils:Using sep_token, but it is not set yet.
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  what type of attraction are you looking for? there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 38.86 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 88.4  success: 79.5  bleu: 17.7    score: 101.6
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [general] [reqmore] <eos_a> <sos_r>  i am sorry, did you mean the restaurant? there are also [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the centre. there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  area food [request]  food <eos_a> <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 52.91 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 87.9  success: 78.5  bleu: 17.4    score: 100.6
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  what type of attraction are you looking for? there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:inference time: 52.43 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 94.0  success: 83.0  bleu: 17.1    score: 105.6
INFO:root:inference time: 51.53 min
INFO:root:inference time: 53.66 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 86.9  success: 76.4  bleu: 16.6    score: 98.2
INFO:root:Scoring time: 0.26 min
INFO:root:validation [CTR] match: 95.5  success: 83.3  bleu: 16.6    score: 106.0
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type choice area [general] [reqmore] <eos_a> <sos_r>  there are [value_choice]  different attractions in the [value_area] . there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:inference time: 41.63 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 87.7  success: 76.8  bleu: 16.7    score: 98.9
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [general] [reqmore] <eos_a> <sos_r>  i am sorry, did you mean the restaurant? there are also [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 44.81 min
INFO:root:inference time: 44.67 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 86.6  success: 74.7  bleu: 16.4    score: 97.0
INFO:root:inference time: 45.07 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 92.7  success: 77.5  bleu: 15.5    score: 100.6
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 90.4  success: 78.3  bleu: 16.3    score: 100.6
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  it is a [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type choice [request]  type <eos_a> <sos_r>  there are [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [nooffer] [inform]  name phone address <eos_a> <sos_r>  another option is [value_name] , another [value_type]  named [value_name] , another 1 is at [value_address] , another is at [value_address] , another is at [value_address] , another is at [value_address] , another is at [value_address] , another is at [value_address] , another is at [value_address] , another is at [value_address] , another is at [value_address] , another is at [value_address] , another is at [value_address] , another is at [value_address] , another
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  food <eos_a> <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:inference time: 38.83 min
INFO:root:Scoring time: 0.30 min
INFO:root:validation [CTR] match: 89.1  success: 63.7  bleu: 11.9    score: 88.3
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointerFalse 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointerFalse 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointerFalse 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointerFalse 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointerFalse 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  what type of attraction are you looking for? there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 25.38 min
INFO:root:Scoring time: 0.37 min
INFO:root:validation [CTR] match: 95.4  success: 85.9  bleu: 17.0    score: 107.6
INFO:root:inference time: 25.84 min
INFO:root:Scoring time: 0.36 min
INFO:root:validation [CTR] match: 94.3  success: 86.9  bleu: 17.7    score: 108.3
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointerFalse 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=True 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointerFalse 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=True 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:['hotel_single', 'hotel_multi']
INFO:root:Reading encoded data from ./experiments_Xdomain/data/hotel-encoded.data.json
INFO:root:train size:3386, dev size:415, test size:394
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 3346
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:['train_single', 'train_multi']
INFO:root:Reading encoded data from ./experiments_Xdomain/data/train-encoded.data.json
INFO:root:train size:3116, dev size:484, test size:495
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:['attraction_single', 'attraction_multi']
INFO:root:Reading encoded data from ./experiments_Xdomain/data/attraction-encoded.data.json
INFO:root:train size:2716, dev size:400, test size:396
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 3709
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 3079
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:['restaurant_single', 'restaurant_multi']
INFO:root:Reading encoded data from ./experiments_Xdomain/data/restaurant-encoded.data.json
INFO:root:train size:3813, dev size:438, test size:437
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:['taxi_single', 'taxi_multi']
INFO:root:Reading encoded data from ./experiments_Xdomain/data/taxi-encoded.data.json
INFO:root:train size:1653, dev size:206, test size:195
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 3394
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 1619
INFO:root:inference time: 9.61 min
INFO:root:Scoring time: 0.07 min
INFO:root:validation [CTR] match: 87.9  success: 77.7  bleu: 17.4    score: 100.1
INFO:root:update eval results to logs_xd/all_0729_sd11_lr0.0001_bs2_ga16-taxi.json
INFO:root:inference time: 14.98 min
INFO:root:Scoring time: 0.17 min
INFO:root:validation [CTR] match: 87.2  success: 75.2  bleu: 17.5    score: 98.7
INFO:root:update eval results to logs_xd/all_0729_sd11_lr0.0001_bs2_ga16-attraction.json
INFO:root:inference time: 16.45 min
INFO:root:Scoring time: 0.20 min
INFO:root:validation [CTR] match: 88.9  success: 82.9  bleu: 16.8    score: 102.8
INFO:root:update eval results to logs_xd/all_0729_sd11_lr0.0001_bs2_ga16-hotel.json
INFO:root:inference time: 17.94 min
INFO:root:Scoring time: 0.35 min
INFO:root:validation [CTR] match: 92.6  success: 86.6  bleu: 18.4    score: 108.0
INFO:root:update eval results to logs_xd/all_0729_sd11_lr0.0001_bs2_ga16-train.json
INFO:root:inference time: 20.15 min
INFO:root:Scoring time: 0.16 min
INFO:root:validation [CTR] match: 90.4  success: 82.9  bleu: 18.3    score: 105.0
INFO:root:update eval results to logs_xd/all_0729_sd11_lr0.0001_bs2_ga16-restaurant.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name]
INFO:root:eos_r not in generated: <sos_a> [attraction] [recommend]  type [inform]  area choice <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . i recommend the [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  what type of attraction are you looking for? there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 84.67 min
INFO:root:inference time: 89.26 min
INFO:root:Scoring time: 0.51 min
INFO:root:validation [CTR] match: 94.3  success: 85.5  bleu: 17.6    score: 107.5
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:Scoring time: 0.49 min
INFO:root:validation [CTR] match: 94.6  success: 80.3  bleu: 15.9    score: 103.3
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 87.88 min
INFO:root:Scoring time: 0.48 min
INFO:root:validation [CTR] match: 96.4  success: 86.5  bleu: 16.9    score: 108.3
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [hotel] [inform]  type [request]  area <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 80.45 min
INFO:root:Scoring time: 0.45 min
INFO:root:validation [CTR] match: 93.0  success: 81.9  bleu: 17.8    score: 105.3
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type choice [request]  type <eos_a> <sos_r>  i have [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 89.23 min
INFO:root:Scoring time: 0.45 min
INFO:root:validation [CTR] match: 94.1  success: 77.1  bleu: 16.3    score: 101.9
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:inference time: 88.19 min
INFO:root:Scoring time: 0.44 min
INFO:root:validation [CTR] match: 95.4  success: 82.3  bleu: 17.2    score: 106.1
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=True 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=True 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [recommend]  type [inform]  area choice <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . i recommend the [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=True 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=True 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the [value_area] . [value_choice]  are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the [value_area] . [value_choice]  are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_a> [general] [reqmore] <eos_a> <sos_r>  i am sorry, did you mean the restaurant? there are also [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name]
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the north. there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the north. there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_a> [general] [reqmore] <eos_a> <sos_r>  i am sorry, did you mean the restaurant? there are also [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  area food choice price <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area] . they serve [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  different attractions in the [value_area] . if you are interested in something specific, we have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i am sorry, there aren't any [value_type]  in that area. i have [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  what type of attraction are you looking for? there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the centre. there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 56.35 min
INFO:root:Scoring time: 0.46 min
INFO:root:validation [CTR] match: 96.4  success: 86.0  bleu: 17.1    score: 108.3
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  museums in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  what type of attraction are you looking for? there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  area food [request]  food <eos_a> <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 54.24 min
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the centre. there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:Scoring time: 0.36 min
INFO:root:validation [CTR] match: 88.4  success: 79.5  bleu: 17.7    score: 101.6
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_r>  yes, [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  what type of attraction are you looking for? there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  food [request]  area <eos_a> <sos_r>  sure, i can help you with that. i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  restaurant -s that meet your criteria. [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 38.86 min
INFO:root:Scoring time: 0.45 min
INFO:root:validation [CTR] match: 91.0  success: 83.7  bleu: 17.6    score: 104.9
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:inference time: 55.10 min
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type choice area [general] [reqmore] <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:Scoring time: 0.34 min
INFO:root:validation [CTR] match: 96.3  success: 84.2  bleu: 16.7    score: 106.9
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_r>  sure, what would you like to do? [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type choice area [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 53.91 min
INFO:root:inference time: 39.11 min
INFO:root:Scoring time: 0.34 min
INFO:root:validation [CTR] match: 88.3  success: 79.1  bleu: 17.4    score: 101.1
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:Scoring time: 0.43 min
INFO:root:validation [CTR] match: 95.4  success: 85.9  bleu: 17.0    score: 107.6
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [select] [inform]  food choice <eos_a> <sos_r>  there are [value_choice]  restaurant -s that meet your criteria. [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, what would you like to do? [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 30.91 min
INFO:root:Scoring time: 0.35 min
INFO:root:validation [CTR] match: 97.2  success: 94.3  bleu: 27.6    score: 123.3
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:inference time: 61.80 min
INFO:root:Scoring time: 0.45 min
INFO:root:validation [CTR] match: 87.6  success: 79.6  bleu: 17.6    score: 101.2
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_r>  sure, [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:inference time: 59.86 min
INFO:root:inference time: 42.16 min
INFO:root:Scoring time: 0.45 min
INFO:root:validation [CTR] match: 95.5  success: 83.3  bleu: 16.6    score: 106.0
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  food [request]  area <eos_a> <sos_r>  sure, i can help you with that. i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:Scoring time: 0.35 min
INFO:root:validation [CTR] match: 92.7  success: 84.5  bleu: 17.4    score: 106.0
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [select] [inform]  food choice <eos_a> <sos_r>  there are [value_choice]  restaurant -s that meet your criteria. [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [general] [reqmore] <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 34.53 min
INFO:root:Scoring time: 0.42 min
INFO:root:validation [CTR] match: 97.4  success: 94.6  bleu: 27.6    score: 123.6
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 37.99 min
INFO:root:Scoring time: 0.34 min
INFO:root:validation [CTR] match: 91.7  success: 85.1  bleu: 17.1    score: 105.5
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:inference time: 42.15 min
INFO:root:eos_r not in generated: <sos_r>  sure, [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:Scoring time: 0.33 min
INFO:root:validation [CTR] match: 90.5  success: 83.9  bleu: 17.1    score: 104.3
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:inference time: 61.31 min
INFO:root:Scoring time: 0.41 min
INFO:root:validation [CTR] match: 87.9  success: 78.5  bleu: 17.4    score: 100.6
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type choice area [general] [reqmore] <eos_a> <sos_r>  there are [value_choice]  different attractions in the [value_area] . there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  colleges in the west. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  name postcode <eos_a> <sos_r>  yes, [value_name]  has a postcode of [value_postcode] . [value_name]  has a postcode of [value_postcode] . [value_name]  has a postcode of [value_postcode] . [value_name]  has a postcode of [value_postcode] . [value_name]  has a postcode of [value_postcode] . [value_name]  has a postcode of [value_postcode] . [value_name]  has a postcode of [value_postcode] . [value_name]  has a post [inform]  of [value_postcode] . [value_name]  has a postcode of [value_postcode]
INFO:root:inference time: 43.59 min
INFO:root:inference time: 47.76 min
INFO:root:Scoring time: 0.45 min
INFO:root:validation [CTR] match: 91.3  success: 84.5  bleu: 16.7    score: 104.6
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:inference time: 43.68 min
INFO:root:Scoring time: 0.44 min
INFO:root:validation [CTR] match: 94.3  success: 86.9  bleu: 17.7    score: 108.3
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type choice area [general] [reqmore] <eos_a> <sos_r>  there are [value_choice]  different attractions in the [value_area] . there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  yes, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:Scoring time: 0.44 min
INFO:root:validation [CTR] match: 93.7  success: 84.9  bleu: 16.7    score: 106.0
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 62.36 min
INFO:root:Scoring time: 0.45 min
INFO:root:validation [CTR] match: 93.6  success: 82.2  bleu: 16.1    score: 104.0
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice] [value_type]  in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the centre. there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 60.20 min
INFO:root:Scoring time: 0.34 min
INFO:root:validation [CTR] match: 86.6  success: 74.7  bleu: 16.4    score: 97.0
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  yes, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type choice area [general] [reqmore] <eos_a> <sos_r>  there are [value_choice]  different attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area] . [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:inference time: 68.06 min
INFO:root:eos_r not in generated: <sos_r>  sure. there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:Scoring time: 0.43 min
INFO:root:validation [CTR] match: 92.7  success: 77.5  bleu: 15.5    score: 100.6
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type choice [request]  type <eos_a> <sos_r>  i have [value_choice]  different attractions. i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 69.96 min
INFO:root:Scoring time: 0.41 min
INFO:root:validation [CTR] match: 93.2  success: 80.4  bleu: 15.9    score: 102.6
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_r>  what would you like to see? there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 70.06 min
INFO:root:eos_r not in generated: <sos_r> [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] .
INFO:root:Scoring time: 0.41 min
INFO:root:validation [CTR] match: 94.0  success: 80.6  bleu: 15.8    score: 103.1
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_r>  there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 69.91 min
INFO:root:inference time: 71.14 min
INFO:root:Scoring time: 0.43 min
INFO:root:validation [CTR] match: 87.8  success: 76.4  bleu: 16.5    score: 98.6
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:Scoring time: 0.45 min
INFO:root:validation [CTR] match: 86.9  success: 76.4  bleu: 16.6    score: 98.2
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  food [request]  food <eos_a> <sos_r>  sure, we have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 71.46 min
INFO:root:Scoring time: 0.43 min
INFO:root:validation [CTR] match: 87.7  success: 76.8  bleu: 16.7    score: 98.9
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  food [request]  food <eos_a> <sos_r>  sure, we have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i am sorry, i don't understand your question. i need a taxi to get me from the restaurant to the restaurant. i need to arrive at the restaurant by the reservation time. <eos_u> <sos_b> [taxi]  destination riverside brasserie departure riverside brasserie arrive 14:30 [attraction]  type entertainment [restaurant]  food
INFO:root:eos_r not in generated: <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  name type choice area [general] [reqmore] <eos_a> <sos_r>  there are [value_choice] [value_type]  in the [value_area] . [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 30.52 min
INFO:root:Scoring time: 0.33 min
INFO:root:validation [CTR] match: 96.9  success: 92.2  bleu: 28.6    score: 123.2
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> <eos_a> <sos_r>  you are booked for 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:
INFO:root:inference time: 39.75 min
INFO:root:eos_r not in generated: <sos_r>  i am sorry, there are [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:Scoring time: 0.43 min
INFO:root:validation [CTR] match: 90.4  success: 80.8  bleu: 17.9    score: 103.5
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:eos_r not in generated: <eos_db> <sos_a> [hotel] [inform]  type [request]  area <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 39.51 min
INFO:root:Scoring time: 0.40 min
INFO:root:validation [CTR] match: 94.0  success: 83.6  bleu: 17.2    score: 106.0
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:inference time: 34.41 min
INFO:root:Scoring time: 0.36 min
INFO:root:validation [CTR] match: 96.6  success: 91.4  bleu: 28.6    score: 122.6
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 36.39 min
INFO:root:Scoring time: 0.33 min
INFO:root:validation [CTR] match: 89.9  success: 80.6  bleu: 17.3    score: 102.6
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:inference time: 42.05 min
INFO:root:Scoring time: 0.30 min
INFO:root:validation [CTR] match: 91.7  success: 81.4  bleu: 17.5    score: 104.0
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:inference time: 42.12 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 89.6  success: 80.4  bleu: 17.2    score: 102.2
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:inference time: 55.87 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 88.8  success: 79.0  bleu: 17.8    score: 101.7
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  name type choice area [general] [reqmore] <eos_a> <sos_r>  there are [value_choice] [value_type]  in the [value_area] . [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 42.72 min
INFO:root:Scoring time: 0.37 min
INFO:root:validation [CTR] match: 90.2  success: 80.6  bleu: 16.9    score: 102.3
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:inference time: 42.65 min
INFO:root:inference time: 54.49 min
INFO:root:Scoring time: 0.37 min
INFO:root:validation [CTR] match: 93.0  success: 82.9  bleu: 16.6    score: 104.6
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:inference time: 55.33 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 88.9  success: 79.1  bleu: 17.8    score: 101.8
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 95.4  success: 80.9  bleu: 17.0    score: 105.1
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 65.75 min
INFO:root:Scoring time: 0.31 min
INFO:root:validation [CTR] match: 95.8  success: 83.1  bleu: 17.4    score: 106.8
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  name postcode [restaurant] [inform]  choice food area <eos_a> <sos_r>  there are [value_choice] [value_food]  restaurant -s in the [value_area] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  post
INFO:root:inference time: 48.09 min
INFO:root:Scoring time: 0.34 min
INFO:root:validation [CTR] match: 92.2  success: 82.1  bleu: 18.2    score: 105.3
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 60.74 min
INFO:root:Scoring time: 0.31 min
INFO:root:validation [CTR] match: 88.4  success: 76.6  bleu: 17.6    score: 100.1
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:inference time: 60.34 min
INFO:root:Scoring time: 0.30 min
INFO:root:validation [CTR] match: 95.4  success: 80.7  bleu: 17.0    score: 105.1
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:inference time: 52.14 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 86.9  success: 73.3  bleu: 16.8    score: 96.9
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:inference time: 59.56 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 87.6  success: 75.6  bleu: 17.5    score: 99.1
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 58.08 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 92.7  success: 80.4  bleu: 16.7    score: 103.2
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 53.90 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 87.2  success: 75.3  bleu: 16.8    score: 98.0
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 60.07 min
INFO:root:inference time: 53.30 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 92.7  success: 77.7  bleu: 16.4    score: 101.6
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 89.3  success: 78.5  bleu: 16.9    score: 100.8
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 59.94 min
INFO:root:inference time: 61.64 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 92.7  success: 79.1  bleu: 16.4    score: 102.3
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 92.7  success: 79.0  bleu: 16.6    score: 102.5
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:inference time: 58.90 min
INFO:root:Scoring time: 0.26 min
INFO:root:validation [CTR] match: 88.0  success: 76.5  bleu: 16.8    score: 99.0
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=True 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=True 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the [value_area] . [value_choice]  are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the [value_area] . [value_choice]  are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the north. there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the north. there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  different attractions in the [value_area] . if you are interested in something specific, we have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  i am sorry, there aren't any [value_type]  in that area. i have [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  i am sorry, i meant place -s to go. i am looking for entertainment in the [value_area] . can you give me the address and postcode for 1 of them? <eos_u> <sos_b> [attraction]  area centre type entertainment [train]  leave 17:45 destination norwich day tuesday departure cambridge people 2 <eos_b> <eos_db>
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i have [value_choice]  different restaurant -s that meet your criteria. would you prefer [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, what would you like to do? [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  i would like to find a restaurant in the centre of town that serves british food. <eos_u> <sos_b> [hotel]  name university arms hotel people 7 day wednesday stay 3 [restaurant]  food british area centre <eos_b> <eos_db> [db_3] <eos_db> <sos_a> [restaurant] [inform]  choice [request]  price <eos_a> <sos_r>  there are [value_choice]  restaurant -s that meet
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  restaurant -s that meet your criteria. would you prefer [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, what would you like to do? [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 16.02 min
INFO:root:eos_r not in generated: <sos_r> [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode.
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 97.2  success: 94.8  bleu: 27.0    score: 123.0
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_r>  sure, [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_r>  i have [value_choice]  different restaurant -s that meet your criteria. they serve [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 16.10 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 97.0  success: 94.5  bleu: 27.0    score: 122.7
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_r>  sure, [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_r>  sure, we have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, what kind of place are you looking for? there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  yes, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  yes, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure. there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r> [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] .
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  different attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  there are [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 15.94 min
INFO:root:Scoring time: 0.26 min
INFO:root:validation [CTR] match: 96.9  success: 92.4  bleu: 27.9    score: 122.5
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  restaurant -s that meet your criteria. would you prefer [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 15.91 min
INFO:root:Scoring time: 0.26 min
INFO:root:validation [CTR] match: 96.8  success: 92.1  bleu: 27.8    score: 122.2
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  we have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [hotel] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  food [request]  food <eos_a> <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 85.84 min
INFO:root:Scoring time: 0.30 min
INFO:root:validation [CTR] match: 93.09  success: 70.87  bleu: 14.01    score: 95.99
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  food [request]  food <eos_a> <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  address postcode phone name <eos_a> <sos_r> [value_name]  chinese food chinese pricerange expensive area centre <eos_b> <eos_db> [db_1] <eos_db> <sos_a> [restaurant] [inform]  name address postcode [offerbook] <eos_a> <sos_r> [value_name]  chinese food chinese pricerange expensive area centre <eos_b> <eos_db> [db_1] <eos_db> <sos_a> [restaurant] [inform]  name address postcode [offerbook] <eos_a> <sos_r> [value_name]  chinese food chinese pricerange expensive area centre name charlie chan is
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  area <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  it is a [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  what type of attraction are you looking for? there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [general] [reqmore] <eos_a> <sos_r>  no, that is all i need. thank you for your help. goodbye. <eos_u> <sos_b> [taxi]  destination galleria departure funky fun house arrive 13:15 [attraction]  type theatre departure funky fun house arrive 13:15 [attraction]  type theatre departure funky fun house arrive 13:15 [attraction]  type theatre departure funky fun house arrive 13:15 [attraction]  type theatre departure the varsity restaurant <eos_b> <eos_db> [db_1] <eos_db>
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 86.35 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 92.50  success: 70.80  bleu: 14.27    score: 95.92
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:['hotel_single', 'hotel_multi']
INFO:root:Reading encoded data from ./experiments_Xdomain/data/hotel-encoded.data.json
INFO:root:train size:3386, dev size:415, test size:394
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 3204
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:['train_single', 'train_multi']
INFO:root:Reading encoded data from ./experiments_Xdomain/data/train-encoded.data.json
INFO:root:train size:3116, dev size:484, test size:495
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 3808
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:['attraction_single', 'attraction_multi']
INFO:root:Reading encoded data from ./experiments_Xdomain/data/attraction-encoded.data.json
INFO:root:train size:2716, dev size:400, test size:396
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 3114
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:['restaurant_single', 'restaurant_multi']
INFO:root:Reading encoded data from ./experiments_Xdomain/data/restaurant-encoded.data.json
INFO:root:train size:3813, dev size:438, test size:437
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 3359
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:['taxi_single', 'taxi_multi']
INFO:root:Reading encoded data from ./experiments_Xdomain/data/taxi-encoded.data.json
INFO:root:train size:1653, dev size:206, test size:195
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 1550
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:['restaurant_single', 'restaurant_multi']
INFO:root:Reading encoded data from ./experiments_Xdomain/data/restaurant-encoded.data.json
INFO:root:train size:3813, dev size:438, test size:437
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:['restaurant_single', 'restaurant_multi']
INFO:root:Reading encoded data from ./experiments_Xdomain/data/restaurant-encoded.data.json
INFO:root:train size:3813, dev size:438, test size:437
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 3359
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 19.49 min
INFO:root:Scoring time: 0.10 min
INFO:root:validation [CTR] match: 94.95  success: 75.51  bleu: 17.01    score: 102.24
INFO:root:update eval results to logs_xd/all_0729_sd11_lr0.0001_bs2_ga16-attraction_test.json
INFO:root:inference time: 21.77 min
INFO:root:Scoring time: 0.11 min
INFO:root:validation [CTR] match: 94.42  success: 80.46  bleu: 15.62    score: 103.06
INFO:root:update eval results to logs_xd/all_0729_sd11_lr0.0001_bs2_ga16-hotel_test.json
INFO:root:inference time: 21.05 min
INFO:root:Scoring time: 0.04 min
INFO:root:validation [CTR] match: 96.92  success: 77.44  bleu: 16.02    score: 103.20
INFO:root:update eval results to logs_xd/all_0729_sd11_lr0.0001_bs2_ga16-taxi_test.json
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 24.98 min
INFO:root:Scoring time: 0.20 min
INFO:root:validation [CTR] match: 95.15  success: 81.01  bleu: 18.32    score: 106.40
INFO:root:update eval results to logs_xd/all_0729_sd11_lr0.0001_bs2_ga16-train_test.json
INFO:root:inference time: 45.25 min
INFO:root:Scoring time: 0.10 min
INFO:root:validation [CTR] match: 94.51  success: 78.95  bleu: 17.71    score: 104.44
INFO:root:update eval results to logs_xd/all_0729_sd11_lr0.0001_bs2_ga16-restaurant_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [recommend]  type [inform]  area choice <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . i recommend the [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 40.13 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 96.40  success: 85.99  bleu: 17.08    score: 108.27
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  name type choice area [general] [reqmore] <eos_a> <sos_r>  there are [value_choice] [value_type]  in the [value_area] . [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [hotel] [inform]  type [request]  area <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 39.46 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 95.80  success: 83.10  bleu: 17.40    score: 106.85
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name]
INFO:root:eos_r not in generated: <sos_a> [attraction] [recommend]  type [inform]  area choice <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . i recommend the [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 38.67 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 96.40  success: 86.49  bleu: 16.89    score: 108.33
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [hotel] [inform]  type [request]  area <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 38.57 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 95.40  success: 82.30  bleu: 17.24    score: 106.09
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  what type of attraction are you looking for? there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 39.42 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 88.39  success: 79.48  bleu: 17.70    score: 101.63
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  food [request]  area <eos_a> <sos_r>  sure, i can help you with that. i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [select] [inform]  food choice <eos_a> <sos_r>  there are [value_choice]  restaurant -s that meet your criteria. [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice] [value_type]  in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 39.32 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 88.80  success: 79.00  bleu: 17.79    score: 101.69
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the centre. there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 38.54 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 87.59  success: 79.58  bleu: 17.59    score: 101.17
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  food [request]  area <eos_a> <sos_r>  sure, i can help you with that. i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 38.67 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 88.40  success: 76.60  bleu: 17.57    score: 100.07
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 40.25 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 96.30  success: 84.18  bleu: 16.70    score: 106.94
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 40.17 min
INFO:root:Scoring time: 0.26 min
INFO:root:validation [CTR] match: 95.40  success: 80.90  bleu: 16.96    score: 105.11
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  area food [request]  food <eos_a> <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 43.82 min
INFO:root:Scoring time: 0.31 min
INFO:root:validation [CTR] match: 95.50  success: 83.28  bleu: 16.63    score: 106.02
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 43.88 min
INFO:root:Scoring time: 0.30 min
INFO:root:validation [CTR] match: 95.40  success: 80.70  bleu: 17.00    score: 105.05
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  what type of attraction are you looking for? there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 39.23 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 88.29  success: 79.08  bleu: 17.39    score: 101.08
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [select] [inform]  food choice <eos_a> <sos_r>  there are [value_choice]  restaurant -s that meet your criteria. [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  colleges in the west. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 39.16 min
INFO:root:Scoring time: 0.26 min
INFO:root:validation [CTR] match: 88.90  success: 79.10  bleu: 17.78    score: 101.78
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=True 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the centre. there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 39.96 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 87.89  success: 78.48  bleu: 17.43    score: 100.61
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 39.93 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 87.60  success: 75.60  bleu: 17.50    score: 99.10
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:inference time: 38.17 min
INFO:root:Scoring time: 0.26 min
INFO:root:validation [CTR] match: 93.59  success: 82.18  bleu: 16.15    score: 104.04
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type choice area [general] [reqmore] <eos_a> <sos_r>  there are [value_choice]  different attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  name type choice area [general] [reqmore] <eos_a> <sos_r>  there are [value_choice] [value_type]  in the [value_area] . [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]  is in the centre, [value_name]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 48.68 min
INFO:root:Scoring time: 0.30 min
INFO:root:validation [CTR] match: 92.70  success: 80.40  bleu: 16.66    score: 103.21
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:inference time: 58.86 min
INFO:root:Scoring time: 0.32 min
INFO:root:validation [CTR] match: 93.19  success: 80.38  bleu: 15.86    score: 102.64
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 45.55 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 92.70  success: 79.00  bleu: 16.64    score: 102.49
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type choice area [general] [reqmore] <eos_a> <sos_r>  there are [value_choice]  different attractions in the [value_area] . there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:inference time: 43.43 min
INFO:root:Scoring time: 0.33 min
INFO:root:validation [CTR] match: 87.69  success: 76.78  bleu: 16.68    score: 98.92
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  food [request]  food <eos_a> <sos_r>  sure, we have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 43.81 min
INFO:root:Scoring time: 0.30 min
INFO:root:validation [CTR] match: 89.30  success: 78.50  bleu: 16.88    score: 100.78
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [general] [reqmore] <eos_a> <sos_r>  i am sorry, did you mean the restaurant? there are also [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 37.71 min
INFO:root:Scoring time: 0.30 min
INFO:root:validation [CTR] match: 86.89  success: 76.38  bleu: 16.61    score: 98.24
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 41.67 min
INFO:root:Scoring time: 0.32 min
INFO:root:validation [CTR] match: 87.20  success: 75.30  bleu: 16.79    score: 98.04
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:inference time: 58.65 min
INFO:root:Scoring time: 0.34 min
INFO:root:validation [CTR] match: 93.99  success: 80.58  bleu: 15.80    score: 103.08
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 51.90 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 92.70  success: 79.10  bleu: 16.40    score: 102.30
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:inference time: 47.46 min
INFO:root:Scoring time: 0.39 min
INFO:root:validation [CTR] match: 92.69  success: 77.48  bleu: 15.54    score: 100.62
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  name postcode [restaurant] [inform]  choice food area <eos_a> <sos_r>  there are [value_choice] [value_food]  restaurant -s in the [value_area] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  postcode [value_postcode] . [value_name]  post
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 60.80 min
INFO:root:Scoring time: 0.38 min
INFO:root:validation [CTR] match: 92.70  success: 77.70  bleu: 16.36    score: 101.56
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type choice area [general] [reqmore] <eos_a> <sos_r>  there are [value_choice]  different attractions in the [value_area] . there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:inference time: 46.64 min
INFO:root:Scoring time: 0.31 min
INFO:root:validation [CTR] match: 87.79  success: 76.38  bleu: 16.54    score: 98.63
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [restaurant] [inform]  food [request]  food <eos_a> <sos_r>  sure, we have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 39.76 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 88.00  success: 76.50  bleu: 16.76    score: 99.01
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=True 
	 use true_db_pointer=False 
	 use true_prev_resp=True 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [general] [reqmore] <eos_a> <sos_r>  i am sorry, did you mean the restaurant? there are also [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area]  area. what type of food would you like? [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 38.64 min
INFO:root:Scoring time: 0.30 min
INFO:root:validation [CTR] match: 86.59  success: 74.67  bleu: 16.35    score: 96.98
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 52.28 min
INFO:root:Scoring time: 0.39 min
INFO:root:validation [CTR] match: 86.90  success: 73.30  bleu: 16.77    score: 96.87
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:inference time: 35.71 min
INFO:root:Scoring time: 0.34 min
INFO:root:validation [CTR] match: 90.99  success: 83.68  bleu: 17.58    score: 104.91
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:inference time: 34.46 min
INFO:root:Scoring time: 0.30 min
INFO:root:validation [CTR] match: 90.40  success: 80.80  bleu: 17.87    score: 103.47
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:inference time: 23.22 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 95.40  success: 85.89  bleu: 16.97    score: 107.62
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:inference time: 22.89 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 94.00  success: 83.60  bleu: 17.22    score: 106.02
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  area food choice price <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area] . they serve [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  museums in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:inference time: 23.20 min
INFO:root:Scoring time: 0.31 min
INFO:root:validation [CTR] match: 92.69  success: 84.48  bleu: 17.44    score: 106.03
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [general] [reqmore] <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 23.15 min
INFO:root:Scoring time: 0.30 min
INFO:root:validation [CTR] match: 91.70  success: 81.40  bleu: 17.47    score: 104.02
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  what type of attraction are you looking for? there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 21.65 min
INFO:root:Scoring time: 0.30 min
INFO:root:validation [CTR] match: 94.29  success: 86.89  bleu: 17.69    score: 108.28
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:inference time: 21.44 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 92.20  success: 82.10  bleu: 18.16    score: 105.31
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type choice area [general] [reqmore] <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 21.65 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 90.49  success: 83.88  bleu: 17.11    score: 104.30
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  name postcode <eos_a> <sos_r>  yes, [value_name]  has a postcode of [value_postcode] . [value_name]  has a postcode of [value_postcode] . [value_name]  has a postcode of [value_postcode] . [value_name]  has a postcode of [value_postcode] . [value_name]  has a postcode of [value_postcode] . [value_name]  has a postcode of [value_postcode] . [value_name]  has a postcode of [value_postcode] . [value_name]  has a post [inform]  of [value_postcode] . [value_name]  has a postcode of [value_postcode]
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  choice price food area [request]  food <eos_a> <sos_r>  there are [value_choice] [value_price]  restaurant -s in the [value_area] . [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:inference time: 21.43 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 89.60  success: 80.40  bleu: 17.23    score: 102.23
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:inference time: 22.94 min
INFO:root:Scoring time: 0.31 min
INFO:root:validation [CTR] match: 93.69  success: 84.88  bleu: 16.70    score: 105.99
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:inference time: 22.62 min
INFO:root:Scoring time: 0.30 min
INFO:root:validation [CTR] match: 93.00  success: 82.90  bleu: 16.65    score: 104.60
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:inference time: 22.86 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 91.29  success: 84.48  bleu: 16.74    score: 104.63
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 22.66 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 90.20  success: 80.60  bleu: 16.90    score: 102.30
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=False 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type choice area [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 21.46 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 91.69  success: 85.09  bleu: 17.06    score: 105.45
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type choice [request]  type <eos_a> <sos_r>  i have [value_choice]  different attractions. i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_a> <eos_a> <sos_r>  you are booked for 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:15, 10:
INFO:root:inference time: 21.48 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 89.90  success: 80.60  bleu: 17.33    score: 102.58
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=True 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the [value_area] . [value_choice]  are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the north. there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  different attractions in the [value_area] . if you are interested in something specific, we have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i am sorry, there aren't any [value_type]  in that area. i have [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  yes, [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  restaurant -s that meet your criteria. [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, what would you like to do? [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 17.16 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 97.40  success: 94.59  bleu: 27.64    score: 123.64
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_r>  sure, [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_r>  yes, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure. there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i am sorry, i don't understand your question. i need a taxi to get me from the restaurant to the restaurant. i need to arrive at the restaurant by the reservation time. <eos_u> <sos_b> [taxi]  destination riverside brasserie departure riverside brasserie arrive 14:30 [attraction]  type entertainment [restaurant]  food
INFO:root:eos_r not in generated: <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  i am sorry, there are [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 16.97 min
INFO:root:Scoring time: 0.27 min
INFO:root:validation [CTR] match: 96.60  success: 91.40  bleu: 28.63    score: 122.63
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=True 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the [value_area] . [value_choice]  are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the north. there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, what would you like to do? [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 16.82 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 97.20  success: 94.29  bleu: 27.58    score: 123.32
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_r>  sure, [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_r>  yes, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the centre. there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  what would you like to see? there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r> [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] .
INFO:root:eos_r not in generated: <sos_r>  there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 16.70 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 96.90  success: 92.20  bleu: 28.61    score: 123.16
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=True 
	 use true_curr_bspn=True 
	 use true_curr_aspn=True 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the [value_area] . [value_choice]  are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the north. there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  different attractions in the [value_area] . if you are interested in something specific, we have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  i am sorry, there aren't any [value_type]  in that area. i have [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i would like to find a restaurant in the centre of town that serves british food. <eos_u> <sos_b> [hotel]  name university arms hotel people 7 day wednesday stay 3 [restaurant]  food british area centre <eos_b> <eos_db> [db_3] <eos_db> <sos_a> [restaurant] [inform]  choice [request]  price <eos_a> <sos_r>  there are [value_choice]  restaurant -s that meet
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  restaurant -s that meet your criteria. would you prefer [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, what would you like to do? [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r> [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode. [value_name]  is the postcode.
INFO:root:eos_r not in generated: <sos_r>  i have [value_choice]  different restaurant -s that meet your criteria. they serve [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 15.17 min
INFO:root:Scoring time: 0.26 min
INFO:root:validation [CTR] match: 97.00  success: 94.49  bleu: 26.95    score: 122.70
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_r>  sure, [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_r>  sure, we have [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, what kind of place are you looking for? there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  yes, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure. there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  restaurant -s that meet your criteria. would you prefer [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 15.00 min
INFO:root:Scoring time: 0.28 min
INFO:root:validation [CTR] match: 96.80  success: 92.10  bleu: 27.76    score: 122.21
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=True 
	 use true_prev_aspn=True 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=True 
	 use true_curr_aspn=True 
	 use_all_previous_context=False
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the [value_area] . [value_choice]  are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  attractions in the north. there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i am sorry, i meant place -s to go. i am looking for entertainment in the [value_area] . can you give me the address and postcode for 1 of them? <eos_u> <sos_b> [attraction]  area centre type entertainment [train]  leave 17:45 destination norwich day tuesday departure cambridge people 2 <eos_b> <eos_db>
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i have [value_choice]  different restaurant -s that meet your criteria. would you prefer [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food]
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  sure, what would you like to do? [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 16.28 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 97.20  success: 94.79  bleu: 26.98    score: 122.98
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_r>  sure, [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_r>  yes, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r> [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] . [value_name]  is located at [value_address] , postcode [value_postcode] .
INFO:root:eos_r not in generated: <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice]  different attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  sure, there is [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <sos_r>  there are [value_choice] [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_r>  there are [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 16.15 min
INFO:root:Scoring time: 0.29 min
INFO:root:validation [CTR] match: 96.90  success: 92.40  bleu: 27.89    score: 122.54
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  area food [request]  food <eos_a> <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 36.92 min
INFO:root:Scoring time: 0.26 min
INFO:root:validation [CTR] match: 95.50  success: 83.28  bleu: 16.63    score: 106.02
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 36.24 min
INFO:root:Scoring time: 0.25 min
INFO:root:validation [CTR] match: 95.40  success: 80.70  bleu: 17.00    score: 105.05
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  area food [request]  food <eos_a> <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 35.55 min
INFO:root:Scoring time: 0.26 min
INFO:root:validation [CTR] match: 90.99  success: 79.38  bleu: 16.63    score: 101.81
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 35.86 min
INFO:root:Scoring time: 0.24 min
INFO:root:validation [CTR] match: 91.50  success: 77.40  bleu: 17.00    score: 101.45
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=True 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 35.60 min
INFO:root:Scoring time: 0.25 min
INFO:root:validation [CTR] match: 96.30  success: 84.18  bleu: 16.70    score: 106.94
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:eos_r not in generated: <eos_db> <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] ,
INFO:root:inference time: 35.55 min
INFO:root:Scoring time: 0.24 min
INFO:root:validation [CTR] match: 95.40  success: 80.90  bleu: 16.96    score: 105.11
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name]
INFO:transformers.tokenization_utils:Model name 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/vocab.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/merges.txt
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/tokenizer_config.json
INFO:root:Reading encoded data from ./data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments/all_0729_sd11_lr0.0001_bs2_ga16/epoch43_trloss0.56_gpt2/pytorch_model.bin
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  there are [value_choice]  nightclubs in the centre. [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [restaurant] [inform]  area food [request]  food <eos_a> <sos_r>  i have [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 48.79 min
INFO:root:Scoring time: 0.60 min
INFO:root:validation [CTR] match: 95.50  success: 83.28  bleu: 16.63    score: 106.02
INFO:root:db total: 7365  right: 6323 acc: 0.86
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type area choice [request]  type <eos_a> <sos_r>  there are [value_choice]  attractions in the [value_area] . there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 48.76 min
INFO:root:Scoring time: 0.58 min
INFO:root:validation [CTR] match: 95.40  success: 80.70  bleu: 17.00    score: 105.05
INFO:root:db total: 7372  right: 6256 acc: 0.85
INFO:root:update eval results to logs_test/all_0729_sd11_lr0.0001_bs2_ga16-all_test.json
INFO:root:Encoding data now and save the encoded data in data/dataset/multiwoz20/data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:Encoding data now and save the encoded data in data/dataset/multiwoz20/data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  yes, there are [value_choice] . [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_a> [restaurant] [select] [inform]  food choice <eos_a> <sos_r>  i have [value_choice]  restaurant -s that meet your criteria. they serve [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:Reading encoded data from data/dataset/multiwoz20/data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  yes, there are [value_choice] . [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_a> [restaurant] [select] [inform]  food choice <eos_a> <sos_r>  i have [value_choice]  restaurant -s that meet your criteria. they serve [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 39.67 min
INFO:root:Scoring time: 0.44 min
INFO:root:validation [CTR] match: 96.10  success: 83.88  bleu: 13.32    score: 103.31
INFO:root:update eval results to logs_test/ubar-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 39.43 min
INFO:root:Scoring time: 0.44 min
INFO:root:validation [CTR] match: 94.40  success: 80.90  bleu: 13.42    score: 101.07
INFO:root:update eval results to logs_test/ubar-all_test.json
INFO:root:Reading encoded data from data/dataset/multiwoz20/data/multi-woz-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Generate setting: 
	 use true_prev_bspn=False 
	 use true_prev_aspn=False 
	 use true_db_pointer=False 
	 use true_prev_resp=False 
	 use true_curr_bspn=False 
	 use true_curr_aspn=False 
	 use_all_previous_context=True
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7365
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  choice name <eos_a> <sos_r>  yes, there are [value_choice] . [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] , [value_name] ,
INFO:root:eos_r not in generated: <sos_a> [restaurant] [select] [inform]  food choice <eos_a> <sos_r>  i have [value_choice]  restaurant -s that meet your criteria. they serve [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] , [value_food] ,
INFO:root:inference time: 39.09 min
INFO:root:Scoring time: 0.45 min
INFO:root:validation [CTR] match: 96.10  success: 83.88  bleu: 13.32    score: 103.31
INFO:root:update eval results to logs_test/ubar-all.json
INFO:root:***** Running Evaluation *****
INFO:root:  Num Turns = 7372
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type [request]  type <eos_a> <sos_r>  sure, there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:eos_r not in generated: <sos_a> [attraction] [inform]  type <eos_a> <sos_r>  there are [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type] , [value_type]
INFO:root:inference time: 41.62 min
INFO:root:Scoring time: 0.43 min
INFO:root:validation [CTR] match: 94.40  success: 80.90  bleu: 13.42    score: 101.07
INFO:root:update eval results to logs_test/ubar-all_test.json
